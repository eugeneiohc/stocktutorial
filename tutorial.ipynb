{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Stock Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Why do we care about stocks?\n",
    "It is inevitable that we will all have to at least touch the market at some point with our investment accounts for retirement, education, etc. It is important to understand at least at a high level what is happening in an industry that virutally all people will be exposed to some point in their life. You do not need to understand how the stock market at a deep level to understand the relevancy of the topic. The simplest concept that one should understand about the application of data science in finance is the rise of [quantitative analysts](https://en.wikipedia.org/wiki/Quantitative_analyst) or quant.\n",
    "### What is quant?\n",
    "The essential definition of quant is the application of statistics and mathematics in finance and more recently the application of data science and machine learning. The traditional role of quant was for [derivative](https://en.wikipedia.org/wiki/Derivative_(finance)) pricing and other asset or risk management calculations. However, recently with the rise of machine learning and data analysis on almost all databases in virtually every industry, people are trying to apply ML and data science to the tradition buying/selling of stocks with quant resulting in a new area called [quant trading](https://www.investopedia.com/terms/q/quantitative-trading.asp). \n",
    "### Some examples and further readings and research into quant trading\n",
    "Quant trading has an extremely long way to go until it can possibly be at the same level as humans. One of quant's biggest flaws and advantages is the separation of emotion or sentiment in its algorithms. Because computers can't process the emotional stress of a stock falling or the excitement of a stock rising, it won't make an irrational decisions, such as panic selling or pouring an unrealistical amount of money into a stock that you think will rise. At the same time however, quant traders have difficulty comprehending events involving a company and understanding whether it will cause a stock to rise or fall. For example, we cann ntake something like Apple and them unveiling a new iPhone that is more expensive than expected. You would have to first see if people will either buy or sell Apple stock in hopes of it doing well or doing poorly and modify a quant algorithm to somehow implement that into its algorithm. Using the following links you can learn more about quant and some applications:\n",
    "- [Quant Trading Wikipedia](https://www.investopedia.com/terms/q/quantitative-trading.asp)\n",
    "- [Basics of Quant](https://www.quantstart.com/articles/Beginners-Guide-to-Quantitative-Trading)\n",
    "\n",
    "The reality is that the quant field is undeveloped and there hasn't been much success in its use, which is why a huge number of companies investing into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial\n",
    "We won't be applying any quant techniques and will be doing much more basic operations using pandas, the quant trading concepts are just ideas to be thinking about when trying to query a bit more on the data. Pandas cannot handle all of the data that is in the dataset that we are using, but later it may be appropriate to do operations on specific ticker symbols or industries for actual application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be working with the last 20 years of stock market data from [Kaggle](https://www.kaggle.com/tsaustin/us-historical-stock-prices-with-earnings-data).\n",
    "This dataset has a gzipped CSV file, ```dataset_summary.csv.gz``` in the first folder with summary statistics (metadata).\n",
    "\n",
    "### Onto the Real Data...\n",
    "```stocks_latest``` folder contains files with the actual market data with the following files (gzippped CSVs)<br><br>\n",
    "```dividends_latest.csv.gz``` has all [dividends](https://www.investopedia.com/terms/d/dividend.asp) of the companies that have had dividend payouts.<br>\n",
    "```earnings_latest.csv.gz``` has all [earnings](https://www.investopedia.com/terms/e/earnings.asp) data attributes like quarters, expected vs actual earnings per share (EPS), release time (post or pre market).\n",
    "```stock_prices_latest.csv.gz``` has stock prices with information such as open price, low, high, trading volume, etc. We will not be doing analysis on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "We will use ```pip3 install -r \"requirements.txt```, which will install all necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DB\n",
    "We will put this information into a SQLite database to demonstrate basic SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the columns\n",
    "We could open up the csvs and manually find the columns, but we can instead use pandas to figure out the columns of each of the files. Pandas allows us to read csvs and with the respective file paths being passed in, we can convert it to a tuple, to later use in string formatting when we create the tables in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('symbol', ' total_prices', ' stock_from_date', ' stock_to_date', ' total_earnings', ' earnings_from_date', ' earnings_to_date')\n",
      "('symbol', ' date', ' dividend')\n",
      "('symbol', ' date', ' qtr', ' eps_est', ' eps', ' release_time')\n"
     ]
    }
   ],
   "source": [
    "def load_columns(summary_filepath, dividends_filepath, earnings_filepath):\n",
    "    # Pandas allows us to read csvs, nrows=0 returns us the top row, which ar the column names\n",
    "    sumcols = tuple(\", \".join(pd.read_csv(summary_filepath, nrows=0, compression='gzip', error_bad_lines=False)).split(','))\n",
    "    dcols = tuple(\", \".join(pd.read_csv(dividends_filepath, nrows=0, compression='gzip', error_bad_lines=False)).split(','))\n",
    "    ecols = tuple(\", \".join(pd.read_csv(earnings_filepath, nrows=0, compression='gzip', error_bad_lines=False)).split(','))\n",
    "    # Printing all of the columns\n",
    "    print(sumcols)\n",
    "    print(dcols)\n",
    "    print(ecols)\n",
    "\n",
    "# These are the paths to the files that we pass in\n",
    "load_columns('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into a SQlite3 DB\n",
    "Now that we know the columns, we can load it into a SQlite3 DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Connection at 0x111c0e650>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_into_db(summary_filepath, dividends_filepath, earnings_filepath):\n",
    "    conn = sqlite3.connect(\":memory:\")\n",
    "    # conn.text_factory = str\n",
    "    c = conn.cursor()\n",
    "    # Create columns again\n",
    "    sumcols = (\", \".join(pd.read_csv(summary_filepath, nrows=0, compression='gzip', error_bad_lines=False)).split(','))\n",
    "    dcols = (\", \".join(pd.read_csv(dividends_filepath, nrows=0, compression='gzip', error_bad_lines=False)).split(','))\n",
    "    ecols = (\", \".join(pd.read_csv(earnings_filepath, nrows=0, compression='gzip', error_bad_lines=False)).split(','))\n",
    "  \n",
    "    # Making the summary table\n",
    "    e = (\"CREATE TABLE summary (%s TEXT, %s INTEGER, %s DATE, %s DATE, %s INTEGER, %s DATE, %s DATE)\" %tuple(sumcols))\n",
    "    c.execute(e)\n",
    "    with gzip.open(summary_filepath, mode=\"rt\", encoding ='utf-8') as f:\n",
    "        dr = csv.DictReader(f)\n",
    "        to_db = [(i['symbol'], i['total_prices'], i['stock_from_date'], i['stock_to_date'], i['total_earnings'], i['earnings_from_date'], i['earnings_to_date']) for i in dr]\n",
    "    e = (\"INSERT INTO summary (%s, %s, %s, %s, %s, %s, %s) VALUES (?, ?, ?, ?, ?, ?, ?);\" %tuple(sumcols))\n",
    "    c.executemany(e, to_db)\n",
    "    \n",
    "    # Making the dividends table\n",
    "    e = (\"CREATE TABLE dividends (%s TEXT, %s DATE, %s FLOAT)\" %tuple(dcols))\n",
    "    c.execute(e)\n",
    "    with gzip.open(dividends_filepath, mode=\"rt\", encoding ='utf-8') as f:\n",
    "        dr = csv.DictReader(f)\n",
    "        to_db = [(i['symbol'], i['date'], i['dividend']) for i in dr]\n",
    "    e = (\"INSERT INTO dividends (%s, %s, %s) VALUES (?, ?, ?);\" %tuple(dcols))\n",
    "    c.executemany(e, to_db)\n",
    "    \n",
    "    # Making the earnings table\n",
    "    e = (\"CREATE TABLE earnings (%s TEXT, %s DATE, %s DATE, %s FLOAT, %s FLOAT, %s TEXT)\" %tuple(ecols))\n",
    "    c.execute(e)\n",
    "    with gzip.open(earnings_filepath, mode=\"rt\", encoding ='utf-8') as f:\n",
    "        dr = csv.DictReader(f)\n",
    "        to_db = [(i['symbol'], i['date'], i['qtr'], i['eps_est'], i['eps'], i['release_time']) for i in dr]\n",
    "    e = (\"INSERT INTO earnings (%s, %s, %s, %s, %s, %s) VALUES (?, ?, ?, ?, ?, ?);\" %tuple(ecols))\n",
    "    c.executemany(e, to_db)\n",
    "    return conn\n",
    "    \n",
    "load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting some basic analysis\n",
    "Now that we know our connection to the sqlite3 server works, and that the tables are in the database. Let's gather some aggregate statistics on the summary data (metadata) of the companies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividend Payouts\n",
    "Dividend payouts are payouts that companies give to investors as a reward for buying their company's shares, some companies decide not to have as many payouts as others but that's another concepts behind business that we do not have to get into. <br>\n",
    "##### How do we do this?\n",
    "We can do a simple SQL operation to find out what companies have historically had the largest dividend pay outs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Dividend Pay Outs and When They Were Paid\n",
      "  Ticker Symbol        Date  Dividend Pay Out in Dollars\n",
      "0         GOOGL  2014-04-03                     567.9717\n",
      "1           GHC  2015-07-01                     425.5000\n",
      "2           KSU  2000-07-13                     181.0000\n",
      "3         NRCIB  2013-05-23                     178.5400\n",
      "4           BBH  2009-04-01                     167.1520\n",
      "5             Y  1998-06-18                     143.4375\n",
      "6           ALX  2012-12-21                     122.0000\n",
      "7           KDP  2018-07-10                     103.7500\n",
      "8         FWONA  2014-07-24                      92.0000\n",
      "9            KF  2008-12-29                      90.3000\n"
     ]
    }
   ],
   "source": [
    "def highest_dividend(conn, num): # cursor is the sqlite3 cursor, num is number of companies we will display\n",
    "    print(\"Highest Dividend Pay Outs and When They Were Paid\")\n",
    "    query = \"\"\"SELECT d.symbol AS \"Ticker Symbol\", d.date AS \"Date\", d.dividend AS \"Dividend Pay Out in Dollars\"\n",
    "                FROM dividends as d\n",
    "                ORDER BY d.dividend DESC\n",
    "                LIMIT %d\"\"\" %num\n",
    "    print(pd.read_sql_query(query, conn, index_col = None))\n",
    "    \n",
    "conn = load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "\n",
    "highest_dividend(conn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is this useful information as an investor?\n",
    "It may be nice to know who has had the high dividend payout but is not a very pracitcal query as it doesn't show us much. Taking this one step further, a skewed statistic of someone having the highest dividend is not useful if as an investor. A company paying out a high dividend in the past dividend does not necessarily mean that they will pay out more later. It could have been the case that the company had a really good quarter or season but now does not see any growth and as a result, an investor should not put money in that company.<br>\n",
    "##### What query would be appropriate for this?\n",
    "As a result, we want to find out if a company has been consistently putting out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dividend Pay Outs and Number of Pay Outs:\n",
      "  Ticker Symbol  Average Dividend Pay out in Dollars  Number of Pay Outs\n",
      "0         GOOGL                           567.971700                   1\n",
      "1            ZG                            58.293500                   1\n",
      "2          MSGN                            47.957100                   1\n",
      "3          HMSY                            47.500100                   1\n",
      "4           OIS                            46.126500                   1\n",
      "5            VC                            43.400000                   1\n",
      "6         DISCB                            41.170000                   1\n",
      "7         DISCK                            39.780000                   1\n",
      "8         FWONA                            39.362367                   3\n",
      "9           PNK                            38.860000                   1\n"
     ]
    }
   ],
   "source": [
    "def highest_avg_dividend(conn, num): # cursor is the sqlite3 cursor, num is number of companies we will display\n",
    "    print(\"Average Dividend Pay Outs and Number of Pay Outs:\")\n",
    "    query = \"\"\"SELECT d.symbol AS \"Ticker Symbol\", AVG(d.dividend) AS \"Average Dividend Pay out in Dollars\", COUNT(d.dividend) AS \"Number of Pay Outs\"\n",
    "                FROM dividends as d\n",
    "                GROUP BY d.symbol\n",
    "                ORDER BY AVG(d.dividend) DESC\n",
    "                LIMIT %d\"\"\" %num\n",
    "    print(pd.read_sql_query(query, conn, index_col = None))\n",
    "    \n",
    "conn = load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "\n",
    "highest_avg_dividend(conn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anything strange?\n",
    "We notice that the number of payouts for all the highest averages are very low! This is not good because the whole point that we did averages was to find out the companies that have the most consistent outputs. <br>\n",
    "There are a couple things that we can do: \n",
    "- we can weigh the higher number of payouts more with some function that values more payouts as more valuable\n",
    "- have a set number of minimum payouts that a company needs to have for us to consider investors for the sake of high payouts\n",
    "We will try both of these strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Weighing higher payouts linearly]\n",
      "  Ticker  Avg Dividend  # of Pay Outs  Linearly Calculated Weight\n",
      "0  HBANP     21.207615             39                    827.0970\n",
      "1    GHC      6.851882             85                    582.4100\n",
      "2  GOOGL    567.971700              1                    567.9717\n",
      "3    ALX      6.705128             39                    261.5000\n",
      "4      Y     15.316964             14                    214.4375\n",
      "5     NC      2.343520             88                    206.2298\n",
      "6    BBH      5.141565             40                    205.6626\n",
      "7  NRCIB      4.053125             48                    194.5500\n",
      "8    KSU      4.997368             38                    189.9000\n",
      "9   GYRO     23.453750              8                    187.6300\n",
      "\n",
      "[Setting Minimum Pay Outs (5)]\n",
      "  Ticker  Avg Dividend  Payouts\n",
      "0   GYRO     23.453750        8\n",
      "1  HBANP     21.207615       39\n",
      "2    TDG     18.916667        6\n",
      "3      Y     15.316964       14\n",
      "4    CKH      8.062456        9\n",
      "5   DHIL      7.181818       11\n",
      "6     KF      6.899980       20\n",
      "7    GHC      6.851882       85\n",
      "8   CTRP      6.788200        5\n",
      "9    ALX      6.705128       39\n"
     ]
    }
   ],
   "source": [
    "def highest_avg_dividend_mod(conn, num, minpay): # cursor is the sqlite3 cursor, num is number of companies we will display\n",
    "    print(\"[Weighing higher payouts linearly]\")\n",
    "    query = \"\"\"SELECT d.symbol AS \"Ticker\", AVG(d.dividend) AS \"Avg Dividend\", COUNT(d.dividend) AS \"# of Pay Outs\", AVG(d.dividend)*COUNT(d.dividend) AS \"Linearly Calculated Weight\"\n",
    "                FROM dividends as d\n",
    "                GROUP BY d.symbol\n",
    "                ORDER BY AVG(d.dividend)*COUNT(d.dividend) DESC\n",
    "                LIMIT %d\"\"\" %num\n",
    "    print(pd.read_sql_query(query, conn, index_col = None))\n",
    "    print()\n",
    "    print(\"[Setting Minimum Pay Outs (%d)]\" %minpay)\n",
    "    query = \"\"\"SELECT d.symbol AS \"Ticker\", AVG(d.dividend) AS \"Avg Dividend\", COUNT(d.dividend) AS \"Payouts\"\n",
    "                FROM dividends as d\n",
    "                GROUP BY d.symbol\n",
    "                HAVING COUNT(d.dividend) >= %d\n",
    "                ORDER BY AVG(d.dividend) DESC\n",
    "                LIMIT %d\"\"\" %(minpay, num)\n",
    "    print(pd.read_sql_query((query), conn, index_col = None))\n",
    "    \n",
    "conn = load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "\n",
    "highest_avg_dividend_mod(conn, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better Result?\n",
    "If we wanted to invest in a company that had high payouts, good thing we didn't buy the companies of strictly highest payouts; we know the companies that we determined in these past two queries are ones that have relatively good payouts over a consistent period. We can modify the parameters as much as we want to get as many payouts as we can. In fact we will do a query on just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Max Payouts]\n",
      "  Ticker  Avg Dividend  Payouts\n",
      "0    PPH      0.100778      301\n",
      "1    GIM      0.055892      287\n",
      "2    HIO      0.050812      270\n",
      "3    CXH      0.048632      269\n",
      "4    KTF      0.064102      269\n",
      "5    AWF      0.100243      268\n",
      "6    NMT      0.062288      266\n",
      "7    CRT      0.190543      265\n",
      "8    DSM      0.044867      265\n",
      "9    MMT      0.040699      265\n"
     ]
    }
   ],
   "source": [
    "def highest_number_dividends(conn, num): # cursor is the sqlite3 cursor, num is number of companies we will display\n",
    "    print(\"[Max Payouts]\")\n",
    "    query = \"\"\"SELECT d.symbol AS \"Ticker\", AVG(d.dividend) AS \"Avg Dividend\", COUNT(d.dividend) AS \"Payouts\"\n",
    "                FROM dividends as d\n",
    "                GROUP BY d.symbol\n",
    "                ORDER BY COUNT(d.dividend) DESC\n",
    "                LIMIT %d\"\"\" %(num)\n",
    "    print(pd.read_sql_query((query), conn, index_col = None))\n",
    "    \n",
    "conn = load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "\n",
    "highest_number_dividends(conn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of Determining Good Dividends\n",
    "The data that this tutorial provides is quite minimal, but the possibilities are limitless when determining the validity and strengh of a certain stock from a company. As shown above, it may be good to buy companies that seem to have very consistent payouts to have more reliablility in dividends if that is what a particular investor is looking for.<br>\n",
    "You are encouraged to think about other ways to analyze dividends using some kind of weighing mechanism or some other way you see fit. It may be appropriate to work with only on company to see dividend trends and be able to analyze other trends, such as company performance over time and factor that into your calculations and education on whether you want to invest in a specific company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earnings Analysis\n",
    "Now in terms of actual investing, it may not be the best idea to only look at dividend payouts, especially if a company is not growing. Even if a company does not pay out dividends, if they are growing over the course of owning a stock, the investor will obtain more capital in the end.<br>\n",
    "An important factor in determing whether a company does well is their quarterly earnings reports, which tell us if a company hits [expectations](https://www.investopedia.com/terms/e/earningsreport.asp) (based on analysts' expectations). If a company continues to impress the public with increasingly hitting beyond expectations, it may attract that company's stock. \n",
    "#### Initial queries\n",
    "We will begin with a very simple query that simply shows us the top earnings of companies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best EPS performance\n",
      "  Ticker Symbol  Quarter  EPS (earnings per share)\n",
      "0          DCIX  06/2014                    987.54\n",
      "1          DCIX  09/2014                    987.54\n",
      "2          NSPR  12/2012                    961.54\n",
      "3          SAEX  12/2013                    918.92\n",
      "4          DRYS  06/2015                    704.00\n",
      "5          DCIX  12/2012                    493.77\n",
      "6          DCIX  12/2014                    493.77\n",
      "7          DCIX  06/2015                    493.77\n",
      "8          SAEX  06/2014                    378.38\n",
      "9          SAEX  06/2015                    378.38\n"
     ]
    }
   ],
   "source": [
    "def highest_earnings(conn, num): # cursor is the sqlite3 cursor, num is number of companies we will display\n",
    "    print(\"Best EPS performance\")\n",
    "    query = \"\"\"SELECT e.symbol AS \"Ticker Symbol\", e.qtr AS \"Quarter\", e.eps AS \"EPS (earnings per share)\"\n",
    "                FROM earnings as e\n",
    "                WHERE e.eps <> 'NULL'\n",
    "                ORDER BY e.eps DESC\n",
    "                LIMIT %d\"\"\" %num\n",
    "    print(pd.read_sql_query(query, conn, index_col = None))\n",
    "    \n",
    "conn = load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "\n",
    "highest_earnings(conn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this useful?\n",
    "As we realized on the last analysis on the dividends, just getting the best earnings report is not necessarily the best piece of evidence to look at for the performance of the company. Similar to the dividends, a company could have one really quarter and destroyed expectations for their earnings, but this does not mean it is consistently beating expectetations \n",
    "#### What should we look at then?\n",
    "We not only need to look at the averages of expectation reports, but we have to look at the difference in terms of how much the company beat the analysts' expectations by. Sometimes companies early on are not profitable but if they are performing better than analysts' expectations, it is valid for an investor to put money into the company since it will grow even if they are techincally not profitable currently. We will now create a query on the averages of how much the stocks beat expectations by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best expected EPS (earnings per share) performance and what quarter it happened\n",
      "  Ticker Symbol      qtr  Amount that beat expectations (dollars)\n",
      "0          INPX  03/2014                                63.518462\n",
      "1          DCIX  09/2012                                49.327000\n",
      "2          SPEX  09/2012                                21.100000\n",
      "3          NSPR  09/2013                                12.110000\n",
      "4          AEZS  09/2012                                 7.567391\n",
      "5           ANY  12/2014                                 6.232727\n",
      "6          NCNA       Q4                                 5.975000\n",
      "7            BH  09/2012                                 5.590000\n",
      "8          PSTV       Q2                                 4.630000\n",
      "9          DAVA  12/2018                                 3.283500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def highest_earnings(conn, num): # cursor is the sqlite3 cursor, num is number of companies we will display\n",
    "    print(\"Best expected EPS (earnings per share) performance and what quarter it happened\")\n",
    "    query = \"\"\"SELECT e.symbol AS \"Ticker Symbol\", e.qtr, AVG(e.eps - e.eps_est) AS \"Amount that beat expectations (dollars)\"\n",
    "                FROM earnings as e\n",
    "                WHERE (e.eps_est <> 'NULL') AND (e.eps <> 'NULL')\n",
    "                GROUP BY e.symbol\n",
    "                ORDER BY AVG(e.eps - e.eps_est) DESC\n",
    "                LIMIT %d\"\"\" %num\n",
    "    print(pd.read_sql_query(query, conn, index_col = None))\n",
    "    print()\n",
    "    \n",
    "conn = load_data_into_db('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "\n",
    "highest_earnings(conn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of Earnings Analysis\n",
    "Again, like the dividend analysis, earnings are similar in that you can start graphing trends and create other factors to consider when thinking of investing in a specific company's stock. Earnings reports also have sentiment tied to them and experience with exposing yourself with people's reactions on earnigns reports may help you make more educated decisions on how much to weigh an earnings report. For example, for the most part, a surpassing of expectations causes people to expect people to buy, which also allows for a margin of short selling, where you take advantage of the spike that people get from an earnings report. \n",
    "##### Other Ideas\n",
    "Other concepts that can be touched upon are seeing if post or pre market release of earnings reports are independent of each other or if companies tend to do post market release or earnings if it is good or bad. <br>\n",
    "You can also explore the concept of people sometimes overvaluing unprofitable companies when their earnings reports show it beating analyst expectations. This dips into the concept of analysts and investors overvaluing startups and other tech companies that are build on hype a lot of the time. Prime examples are when [unicorns](https://www.investopedia.com/terms/u/unicorn.asp) IPO and valuations are overvalued, or companies such as [Tesla](https://en.wikipedia.org/wiki/Tesla,_Inc.) whose stocks are truly built on mostly hype and not actual valuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "We will now make some visualizations of important information about these companies and statistics that we may want to consider. We will use pandas dataframes to easy use [matplotlib](http://matplotlib.org) to graph them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up data\n",
    "We have to load up the data as a dataframe via pandas, since we need to graph it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     symbol  total_prices stock_from_date stock_to_date  total_earnings  \\\n",
       " 0         A          4962      1999-11-18    2019-08-09              42   \n",
       " 1        AA           697      2016-11-01    2019-08-09              11   \n",
       " 2      AAAP           574      2015-11-11    2018-07-18               0   \n",
       " 3      AABA          5434      1998-01-02    2019-08-07              14   \n",
       " 4       AAC          1222      2014-10-02    2019-08-09              21   \n",
       " 5       AAL          3489      2005-09-27    2019-08-07              23   \n",
       " 6      AAMC          1675      2012-12-13    2019-08-09              23   \n",
       " 7      AAME          5434      1998-01-02    2019-08-07              39   \n",
       " 8       AAN          5436      1998-01-02    2019-08-09              41   \n",
       " 9      AAOI          1476      2013-09-26    2019-08-07              24   \n",
       " 10     AAON          5434      1998-01-02    2019-08-07              41   \n",
       " 11      AAP          4454      2001-11-29    2019-08-09              42   \n",
       " 12     AAPL          5434      1998-01-02    2019-08-07              41   \n",
       " 13      AAT          2157      2011-01-13    2019-08-09              34   \n",
       " 14      AAU          3432      2005-12-20    2019-08-09              30   \n",
       " 15      AAV          3601      2004-03-31    2018-07-19               0   \n",
       " 16     AAWW          3458      2005-11-09    2019-08-07              42   \n",
       " 17     AAXJ          2763      2008-08-15    2019-08-07               0   \n",
       " 18     AAXN          4571      2001-06-07    2019-08-08              23   \n",
       " 19       AB          5436      1998-01-02    2019-08-09              41   \n",
       " 20     ABAC          2205      2010-07-20    2019-04-23               0   \n",
       " 21     ABAX          5180      1998-01-02    2018-08-03               0   \n",
       " 22      ABB          4614      2001-04-06    2019-08-09              35   \n",
       " 23     ABBV          1663      2013-01-02    2019-08-09              26   \n",
       " 24      ABC          5436      1998-01-02    2019-08-09              41   \n",
       " 25     ABCB          5434      1998-01-02    2019-08-07              41   \n",
       " 26     ABCD          2272      2009-12-09    2018-12-18               0   \n",
       " 27     ABDC          1321      2014-05-09    2019-08-07              20   \n",
       " 28      ABE          4358      2001-01-02    2018-06-06               0   \n",
       " 29     ABEO          5434      1998-01-02    2019-08-07              19   \n",
       " ...     ...           ...             ...           ...             ...   \n",
       " 7061   ZFGN          1293      2014-06-19    2019-08-07              21   \n",
       " 7062     ZG          2026      2011-07-20    2019-08-07              23   \n",
       " 7063   ZGNX          2190      2010-11-23    2019-08-07              35   \n",
       " 7064   ZION          5434      1998-01-02    2019-08-07              41   \n",
       " 7065  ZIONW          2311      2010-05-20    2019-08-06               0   \n",
       " 7066  ZIONZ           906      2014-09-26    2018-11-14               0   \n",
       " 7067   ZIOP          3512      2005-08-24    2019-08-07              42   \n",
       " 7068    ZIV          2186      2010-11-30    2019-08-07               0   \n",
       " 7069   ZIXI          5434      1998-01-02    2019-08-07              41   \n",
       " 7070   ZKIN           485      2017-09-01    2019-08-07               0   \n",
       " 7071   ZLAB           473      2017-09-20    2019-08-07               1   \n",
       " 7072     ZM            80      2019-04-18    2019-08-12               1   \n",
       " 7073     ZN          3171      2007-01-03    2019-08-07              29   \n",
       " 7074   ZNGA          1921      2011-12-16    2019-08-07              31   \n",
       " 7075    ZNH          5431      1998-01-02    2019-08-02              15   \n",
       " 7076   ZOES          1058      2014-04-11    2018-06-22               0   \n",
       " 7077    ZOM           763      2016-07-29    2019-08-09               3   \n",
       " 7078     ZS           355      2018-03-16    2019-08-13               2   \n",
       " 7079   ZSAN          1141      2015-01-27    2019-08-07              17   \n",
       " 7080  ZTEST          1966      2011-10-05    2019-07-31               0   \n",
       " 7081    ZTO           695      2016-10-27    2019-08-02              12   \n",
       " 7082    ZTR          5431      1998-01-02    2019-08-02               0   \n",
       " 7083    ZTS          1637      2013-02-01    2019-08-02              26   \n",
       " 7084   ZUMZ          3588      2005-05-06    2019-08-07              41   \n",
       " 7085    ZUO           338      2018-04-11    2019-08-13               2   \n",
       " 7086    ZVO          2600      2009-04-15    2019-08-12               2   \n",
       " 7087     ZX          1784      2011-05-16    2018-07-19               0   \n",
       " 7088   ZYME           570      2017-04-28    2019-08-02              10   \n",
       " 7089   ZYNE          1009      2015-08-05    2019-08-07              17   \n",
       " 7090   ZYXI          4157      2002-12-31    2019-08-06               3   \n",
       " \n",
       "      earnings_from_date earnings_to_date  \n",
       " 0            2009-05-14       2019-08-14  \n",
       " 1            2017-01-24       2019-07-17  \n",
       " 2                   NaN              NaN  \n",
       " 3            2014-01-28       2017-04-18  \n",
       " 4            2014-11-05       2019-04-16  \n",
       " 5            2014-01-28       2019-07-25  \n",
       " 6            2013-05-09       2018-11-07  \n",
       " 7            2009-05-14       2018-11-13  \n",
       " 8            2009-08-05       2019-07-25  \n",
       " 9            2013-11-07       2019-08-07  \n",
       " 10           2009-05-06       2019-05-02  \n",
       " 11           2009-05-20       2019-08-13  \n",
       " 12           2009-07-21       2019-07-30  \n",
       " 13           2011-05-10       2019-07-30  \n",
       " 14           2011-08-11       2018-11-07  \n",
       " 15                  NaN              NaN  \n",
       " 16           2009-05-05       2019-08-01  \n",
       " 17                  NaN              NaN  \n",
       " 18           2014-02-26       2019-08-08  \n",
       " 19           2009-07-30       2019-07-25  \n",
       " 20                  NaN              NaN  \n",
       " 21                  NaN              NaN  \n",
       " 22           2009-10-30       2019-07-25  \n",
       " 23           2013-04-26       2019-07-26  \n",
       " 24           2009-07-30       2019-08-01  \n",
       " 25           2009-07-21       2019-07-26  \n",
       " 26                  NaN              NaN  \n",
       " 27           2014-11-10       2019-08-07  \n",
       " 28                  NaN              NaN  \n",
       " 29           2015-05-14       2019-08-09  \n",
       " ...                 ...              ...  \n",
       " 7061         2014-08-13       2019-08-08  \n",
       " 7062         2014-02-12       2019-08-07  \n",
       " 7063         2011-03-03       2019-08-06  \n",
       " 7064         2009-07-20       2019-07-22  \n",
       " 7065                NaN              NaN  \n",
       " 7066                NaN              NaN  \n",
       " 7067         2009-05-15       2019-08-08  \n",
       " 7068                NaN              NaN  \n",
       " 7069         2009-07-28       2019-08-01  \n",
       " 7070                NaN              NaN  \n",
       " 7071         2019-03-07       2019-03-07  \n",
       " 7072         2019-06-06       2019-06-06  \n",
       " 7073         2009-05-15       2018-05-08  \n",
       " 7074         2012-02-14       2019-07-31  \n",
       " 7075         2010-10-27       2017-03-31  \n",
       " 7076                NaN              NaN  \n",
       " 7077         2018-08-09       2019-08-08  \n",
       " 7078         2019-02-28       2019-05-30  \n",
       " 7079         2015-05-11       2019-08-14  \n",
       " 7080                NaN              NaN  \n",
       " 7081         2016-11-28       2019-08-15  \n",
       " 7082                NaN              NaN  \n",
       " 7083         2013-04-30       2019-08-06  \n",
       " 7084         2009-05-21       2019-06-06  \n",
       " 7085         2019-03-21       2019-05-30  \n",
       " 7086         2019-05-09       2019-08-07  \n",
       " 7087                NaN              NaN  \n",
       " 7088         2017-05-15       2019-08-02  \n",
       " 7089         2015-08-27       2019-08-06  \n",
       " 7090         2019-02-26       2019-07-31  \n",
       " \n",
       " [7091 rows x 7 columns],        symbol        date  dividend\n",
       " 0        MSFT  2016-11-15     0.390\n",
       " 1        MSFT  2011-05-17     0.160\n",
       " 2        MSFT  2008-05-13     0.110\n",
       " 3        MSFT  2011-02-15     0.160\n",
       " 4        MSFT  2012-02-14     0.200\n",
       " 5        MSFT  2011-08-16     0.160\n",
       " 6        MSFT  2006-02-15     0.090\n",
       " 7        MSFT  2014-11-18     0.310\n",
       " 8        MSFT  2015-05-19     0.310\n",
       " 9        MSFT  2008-11-18     0.130\n",
       " 10       MSFT  2016-02-16     0.360\n",
       " 11       MSFT  2005-05-16     0.080\n",
       " 12       MSFT  2004-08-23     0.080\n",
       " 13       MSFT  2007-11-13     0.110\n",
       " 14       MSFT  2006-11-14     0.100\n",
       " 15       MSFT  2010-05-18     0.130\n",
       " 16       MSFT  2016-08-16     0.360\n",
       " 17       MSFT  2015-08-18     0.310\n",
       " 18       MSFT  2011-11-15     0.200\n",
       " 19       MSFT  2005-11-15     0.080\n",
       " 20       MSFT  2007-02-13     0.100\n",
       " 21       MSFT  2017-08-15     0.390\n",
       " 22       MSFT  2014-02-18     0.280\n",
       " 23       MSFT  2012-08-14     0.200\n",
       " 24       MSFT  2015-11-17     0.360\n",
       " 25       MSFT  2016-05-17     0.360\n",
       " 26       MSFT  2013-05-14     0.230\n",
       " 27       MSFT  2012-11-13     0.230\n",
       " 28       MSFT  2015-02-17     0.310\n",
       " 29       MSFT  2005-02-15     0.080\n",
       " ...       ...         ...       ...\n",
       " 228089    NMY  2019-08-14     0.044\n",
       " 228090    NMZ  2019-08-14     0.060\n",
       " 228091    NNC  2019-08-14     0.039\n",
       " 228092    NNY  2019-08-14     0.030\n",
       " 228093    NOM  2019-08-14     0.043\n",
       " 228094     NP  2019-08-14     0.450\n",
       " 228095    NPN  2019-08-14     0.041\n",
       " 228096    NPV  2019-08-14     0.044\n",
       " 228097    NQP  2019-08-14     0.051\n",
       " 228098    NRK  2019-08-14     0.045\n",
       " 228099    NRP  2019-08-06     0.450\n",
       " 228100    NRT  2019-08-15     0.220\n",
       " 228101     NS  2019-08-06     0.600\n",
       " 228102    NSL  2019-08-14     0.038\n",
       " 228103    NTC  2019-08-14     0.041\n",
       " 228104    NTX  2019-08-14     0.045\n",
       " 228105    NUM  2019-08-14     0.045\n",
       " 228106    NUO  2019-08-14     0.042\n",
       " 228107    NUV  2019-08-14     0.031\n",
       " 228108    NUW  2019-08-14     0.056\n",
       " 228109    NVG  2019-08-14     0.066\n",
       " 228110    NXC  2019-08-14     0.044\n",
       " 228111    NXJ  2019-08-14     0.055\n",
       " 228112    NXN  2019-08-14     0.040\n",
       " 228113    NXP  2019-08-14     0.046\n",
       " 228114    NXQ  2019-08-14     0.042\n",
       " 228115    NXR  2019-08-14     0.044\n",
       " 228116   NYCB  2019-08-09     0.170\n",
       " 228117    NYV  2019-08-14     0.043\n",
       " 228118    NZF  2019-08-14     0.066\n",
       " \n",
       " [228119 rows x 3 columns],        symbol        date      qtr  eps_est   eps release_time\n",
       " 0           A  2009-05-14  04/2009      NaN   NaN         post\n",
       " 1           A  2009-08-17  07/2009      NaN   NaN         post\n",
       " 2           A  2009-11-13  10/2009      NaN   NaN          pre\n",
       " 3           A  2010-02-12  01/2010      NaN   NaN          pre\n",
       " 4           A  2010-05-17  04/2010      NaN   NaN         post\n",
       " 5           A  2010-08-16  07/2010      NaN   NaN         post\n",
       " 6           A  2010-11-12  10/2010      NaN   NaN          pre\n",
       " 7           A  2011-02-14  01/2011      NaN   NaN         post\n",
       " 8           A  2011-05-13  04/2011      NaN   NaN          NaN\n",
       " 9           A  2011-08-15  07/2011      NaN   NaN         post\n",
       " 10          A  2011-11-15  10/2011      NaN   NaN         post\n",
       " 11          A  2012-02-15  01/2012      NaN   NaN         post\n",
       " 12          A  2012-05-14  04/2012      NaN   NaN         post\n",
       " 13          A  2012-08-15  07/2012      NaN   NaN         post\n",
       " 14          A  2012-11-19  10/2012    0.800  0.84         post\n",
       " 15          A  2013-02-14  01/2013    0.660  0.63         post\n",
       " 16          A  2013-05-14  04/2013    0.670  0.77         post\n",
       " 17          A  2013-08-14  07/2013    0.620  0.68         post\n",
       " 18          A  2013-11-14  10/2013    0.760  0.81         post\n",
       " 19          A  2014-02-13  01/2014    0.660  0.67         post\n",
       " 20          A  2014-05-14  04/2014      NaN  0.72         post\n",
       " 21          A  2014-08-14  07/2014    0.740  0.78         post\n",
       " 22          A  2014-11-17  10/2014    0.500  0.88         post\n",
       " 23          A  2015-02-17  01/2015    0.410  0.41         post\n",
       " 24          A  2015-05-18  04/2015    0.390  0.38         post\n",
       " 25          A  2015-08-17  07/2015    0.410  0.44         post\n",
       " 26          A  2015-11-16  10/2015    0.470  0.50         post\n",
       " 27          A  2016-02-16  01/2016    0.430  0.46         post\n",
       " 28          A  2016-05-16  04/2016    0.390  0.44         post\n",
       " 29          A  2016-08-17  07/2016    0.470  0.49         post\n",
       " ...       ...         ...      ...      ...   ...          ...\n",
       " 141948   ZYME  2017-05-15  03/2017      NaN -1.13          NaN\n",
       " 141949   ZYME  2017-08-08  06/2017   -0.620 -0.63         post\n",
       " 141950   ZYME  2017-11-08  09/2017   -0.740 -0.65          NaN\n",
       " 141951   ZYME  2018-03-14  12/2017      NaN  1.28          NaN\n",
       " 141952   ZYME  2018-05-01  03/2018   -0.680 -0.83          NaN\n",
       " 141953   ZYME  2018-08-01  06/2018   -0.350 -0.22          NaN\n",
       " 141954   ZYME  2018-11-06  09/2018   -0.630 -0.59          NaN\n",
       " 141955   ZYME  2019-03-06       Q4    0.101  0.29          NaN\n",
       " 141956   ZYME  2019-05-02       Q1   -0.540 -0.43          NaN\n",
       " 141957   ZYME  2019-08-02       Q2   -0.664 -0.89          NaN\n",
       " 141958   ZYNE  2015-08-27  06/2015      NaN -0.80          NaN\n",
       " 141959   ZYNE  2015-11-11  09/2015   -0.410 -0.44          NaN\n",
       " 141960   ZYNE  2016-03-14  12/2015   -0.620 -0.62          NaN\n",
       " 141961   ZYNE  2016-05-12  03/2016   -0.540 -0.49          NaN\n",
       " 141962   ZYNE  2016-08-11  06/2016   -0.530 -0.70          NaN\n",
       " 141963   ZYNE  2016-11-14  09/2016   -0.670 -0.67          NaN\n",
       " 141964   ZYNE  2017-03-27  12/2016   -0.730 -0.71          pre\n",
       " 141965   ZYNE  2017-05-09  03/2017   -0.550 -0.60          pre\n",
       " 141966   ZYNE  2017-08-01  06/2017   -0.600 -0.64          NaN\n",
       " 141967   ZYNE  2017-11-14  09/2017   -0.590 -0.63          NaN\n",
       " 141968   ZYNE  2018-03-12  12/2017   -0.610 -0.60          NaN\n",
       " 141969   ZYNE  2018-05-08  03/2018   -0.640 -0.91          NaN\n",
       " 141970   ZYNE  2018-08-02  06/2018   -0.770 -0.89          pre\n",
       " 141971   ZYNE  2018-11-08  09/2018   -0.690 -0.47          NaN\n",
       " 141972   ZYNE  2019-03-11       Q4   -0.646 -0.44          NaN\n",
       " 141973   ZYNE  2019-05-08       Q1   -0.488 -0.47          NaN\n",
       " 141974   ZYNE  2019-08-06       Q2   -0.497 -0.50          pre\n",
       " 141975   ZYXI  2019-02-26       Q4    0.073  0.08          NaN\n",
       " 141976   ZYXI  2019-04-30       Q1    0.050  0.07          NaN\n",
       " 141977   ZYXI  2019-07-31       Q2    0.060  0.06          NaN\n",
       " \n",
       " [141978 rows x 6 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_pandas(summary_filepath, dividends_filepath, earnings_filepath):\n",
    "    sum_df = pd.read_csv(summary_filepath, compression='gzip', error_bad_lines=False)\n",
    "    div_df = pd.read_csv(dividends_filepath, compression='gzip', error_bad_lines=False)\n",
    "    ear_df = pd.read_csv(earnings_filepath, compression='gzip', error_bad_lines=False)\n",
    "    # We return the data as dataframes to use matplotlib\n",
    "    return (sum_df, div_df, ear_df)\n",
    "\n",
    "load_data_pandas('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Example\n",
    "We will use matplotlib to give an example of data that we can graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD4CAYAAAAUymoqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAf30lEQVR4nO3df5RcZZ3n8feHpsEGf3QY+jChk5jghOwS2U2gD7An6o6IBNjRhLgjweOKDscMO3BGd2YzEwaPsDPOIRoVxxkHT1SOsiqIgiE7oBGMjns8G6RjQkLASIIgaSNEMeBKb0ia7/5Rt8Lt7qruvl11q25VfV7n9Omq595b9eS5qfr2fb7PfR5FBGZmZlkc0+wKmJlZ63HwMDOzzBw8zMwsMwcPMzPLzMHDzMwyO7bZFajVySefHHPnzm12NczMWsrWrVt/FRF90z2+5YPH3LlzGRwcbHY1zMxaiqQnazne3VZmZpaZg4eZmWXm4GFmZpk5eJiZWWZ1CR6SbpH0jKSHU2UnSbpP0mPJ7xlJuSR9WtIeSTsknZU65opk/8ckXVGPupmZWf3V68rji8BFY8rWAN+NiPnAd5PnABcD85OfVcDNUAo2wPXAucA5wPXlgGPFsWHbEEvWbmbemntYsnYzG7YNNbtKZtYEdQkeEfED4NkxxcuALyWPvwQsT5XfGiVbgF5JM4GlwH0R8WxE/Aa4j/EByZpow7Yhrr1rJ0MHhwlg6OAw19610wHErAPlmfM4JSL2J49/CZySPO4Hnkrtty8pq1Y+jqRVkgYlDR44cKC+tbaq1m3azfDhkVFlw4dHWLdpd5NqZGbN0pCEeZQWDanbwiERsT4iBiJioK9v2jdIWka/ODicqdzM2leewePppDuK5PczSfkQMDu136ykrFq5FcSpvT2Zys2sfeUZPDYC5RFTVwB3p8rfk4y6Og94Lune2gRcKGlGkii/MCmzgli9dAE93V2jynq6u1i9dEGTamRmzVKXua0k3Qb8IXCypH2URk2tBe6QdCXwJPDOZPd7gUuAPcALwPsAIuJZSX8HPJjs97cRMTYJb020fHEpBbVu025+cXCYU3t7WL10wdFyM+scavU1zAcGBsITI5qZZSNpa0QMTPd432FuZmaZOXiYmVlmDh5mZpaZg4eZmWXm4GFmZpk5eJiZWWYOHmZmlpmDh5mZZebgYWZmmdVlehIzs3a3YduQp+ZJcfAwM5tEeSG08no25YXQgI4NIO62MjObhBdCG8/Bw8xsEl4IbTwHDzOzSXghtPEcPMzMJuGF0MZzwtzMbBJeCG28XIOHpAXA11JFpwEfBnqB9wMHkvK/iYh7k2OuBa4ERoA/jwgvRWtmTbd8cX9HB4uxcg0eEbEbWAQgqQsYAr5JaenZmyLi4+n9JZ0BrAQWAqcC90s6PSJGD3MwM7OmamTO4y3A3oh4coJ9lgG3R8ShiPgZpXXOz2lI7czMbMoaGTxWArelnl8jaYekWyTNSMr6gadS++xLykaRtErSoKTBAwcOjN1sZmY5a0jwkHQc8Hbg60nRzcDrKHVp7Qc+keX1ImJ9RAxExEBfX19d62pmZpNr1JXHxcCPI+JpgIh4OiJGIuIl4HO83DU1BMxOHTcrKTMzswJpVPC4nFSXlaSZqW2XAg8njzcCKyUdL2keMB/4UYPqaGZmU5T7fR6STgTeCvxpqvhjkhYBATxR3hYRuyTdATwCHAGu9kgrs9bkWWjzUZR2VUQ0/E3raWBgIAYHB5tdDTNLGTsLLZTuyL5xxZkOIDWoZ7tK2hoRA9Oti6cnMbO68yy0+ShSuzp4mFndeRbafBSpXR08zKzuPAttPorUrg4eZlZ3noU2H0VqV8+qa2Z151lo81GkdvVoKzOzDuTRVmZm1nDutjLrYEW54cxaj4OHWYcae8PZ0MFhrr1rJ4ADiE3K3VZmHapIN5xZ63HwMOtQRbrhzFqPg4dZhyrSDWfWehw8zDpUkW44s9bjhLlZhyrSDWfWehw8zDrY8sX9DhY2LQ4ebcxj+M0sL7nnPCQ9IWmnpO2SBpOykyTdJ+mx5PeMpFySPi1pj6Qdks7Ku37tqjyGf+jgMMHLY/g3bPOS8GZWu0Zdebw5In6Ver4G+G5ErJW0Jnn+18DFlNYtnw+cC9yc/LaMJhrD76uP5vjQhp3c9sBTjETQJXH5ubP5yPIzm12twnO7FVOzRlstA76UPP4SsDxVfmuUbAF6Jc1sRgVbncfwF8uHNuzky1t+zkgyEelIBF/e8nM+tGFnk2tWbG634mpE8AjgO5K2SlqVlJ0SEfuTx78ETkke9wNPpY7dl5SNImmVpEFJgwcOHMir3i3NY/iL5bYHnspUbiVut+JqRPB4Q0ScRalL6mpJb0pvjNKc8JnmhY+I9RExEBEDfX19daxq+/AY/mIZqbL0QbVyK3G7FVfuOY+IGEp+PyPpm8A5wNOSZkbE/qRb6plk9yFgdurwWUmZZeQx/NYOuqSKgaJLakJtLC3X4CHpROCYiPht8vhC4G+BjcAVwNrk993JIRuBayTdTilR/lyqe8sy8hh+a3WXnzubL2/5ecVya668rzxOAb6p0l8JxwJfjYhvS3oQuEPSlcCTwDuT/e8FLgH2AC8A78u5fmYN0d/bw1CFwQr9zkFNqDyqyqOtisfL0Jo1wNi1M6CUg7pxxZm+OrSmqHUZWt9hbtYAzkFZu3HwMGsQ56CsnXhKdjMzy8zBw8zMMnPwMDOzzBw8zMwsMwcPMzPLzMHDzMwyc/AwM7PMHDzMzCwzBw8zM8vMwcPMzDJz8DAzs8wcPMzMLDMHDzMzy8zBw8zMMssteEiaLel7kh6RtEvSB5LyGyQNSdqe/FySOuZaSXsk7Za0NK+6mZlZbfJcz+MI8JcR8WNJrwK2Srov2XZTRHw8vbOkM4CVwELgVOB+SadHxAhmZlYouQWPiNgP7E8e/1bSo8BEK+EsA26PiEPAzyTtAc4B/k9edbTabdg25NXxzDpQQ3IekuYCi4EHkqJrJO2QdIukGUlZP/BU6rB9VAk2klZJGpQ0eODAgZxqbZMpr8s9dHCYAIYODnPtXTvZsG2o2VUzs5zlHjwkvRK4E/hgRDwP3Ay8DlhE6crkE1lfMyLWR8RARAz09fXVtb42des27Wb48OhexeHDI6zbtLtJNTKzRsk1eEjqphQ4vhIRdwFExNMRMRIRLwGfo9Q1BTAEzE4dPisps4L6xcHhTOVm1j7yHG0l4AvAoxHxyVT5zNRulwIPJ483AislHS9pHjAf+FFe9bPandrbk6nc6m/DtiGWrN3MvDX3sGTtZncZWsPkeeWxBPgvwPljhuV+TNJOSTuANwP/DSAidgF3AI8A3wau9kirYlu9dAE93V2jynq6u1i9dEGTatRZnHOyZlJENLsONRkYGIjBwcFmV6NjebRV8yxZu5mhCl2E/b09/HDN+U2okbUSSVsjYmC6x+d5n4d1gOWL+x0smsQ5J2smT09i1qKcc7JmcvAwa1Grly6gu0ujyrq75JyTNYSDh1krG5uybO0UprUQ5zzMWkx5kEKlZPnhl4J1m3Y7D2W5c/AwK5DJRq+Vh+eOvbM/zQlzawQHD7OCGBsYyvdtAEcDSKUpYcZywtwawTkPs4KYylxhk11V+CZNaxQHD7OCmMp9GxNdVfT39nDjijOd77CGcPAwK4hqgeEY6eiUI9WmhPnUZYv44ZrzHTisYZzzMCuIE46r/LfcSASrv/EQMDr34SlhrJk8t5VZA1QaRQUvB4GpfApnnNDNtg9fmG9FrWN4biuzgqs0imr1Nx6CKN2XMVW/eeFwXlU0y8zBwyxnlUZRHR5p7St+MyfMzXLmm/asHTl4mOWs94Tu+rxOT31ex6weCtdtJeki4B+ALuDzEbG23u/R7AWMNmwb4n/8r11H+7B7e7q54e0LO3bETHqupi6JkYijv/trOD9ZzvNkCe1qU4Wkt7/53/Rxz479ueQmuo8RN7x9Yd1ftxU0+/NqlRVqtJWkLuCnwFuBfcCDwOUR8Ui1Y7KOtqo0N1BPd1fDbq7asG2I1d94aFyfd/cxYt0f//uO+1BMZa6m6ZyfLOe50r7dxwg0OjfRfYx45SuO5eALh+k9oZv/+/+OZEp4T1ctAbTVNfvz2s5qHW1VtG6rc4A9EfF4RLwI3A4sq+cbTGUKiDyt27S7YrK0PBtqp5nKXE3TOT9ZznPFhPZLMe48HX4p+M0LhwlKI5/yDhzvPm8OT6z9Tx1981+zP69WXdG6rfqBp1LP9wHnjt1J0ipgFcCcOXMyvUGzl+6c6H06MbE61X9z1rbJcp6L1u4nHtfF31/qv6yh+Z9Xq65oVx5TEhHrI2IgIgb6+voyHdvspTsnep9OnA11qv/mrG0zlfO8YdsQS9ZuLtT6Sd3HyIEjpdmfV6uuaMFjCJidej4rKaubanMDNWom0kpLh0LpS6MTZ0OtdD7Gms75mew8l/vSKy2o1Eyd2n1ZTbM/r1Zd0bqtHgTmS5pHKWisBN5Vzzdo9txA5ffxaKuS9Pmo52iryc7zVHItZQJe09PN71480pCb+9wl87Jmf16tukKNtgKQdAnwKUpDdW+JiL+faH/PbWXTMW/NPVPqrurv7eGHa84HSlcrf3nHQ4zk/JlJv6dZXtpubquIuBe4t9n1sPZ2am/PpF1WlbpH8g4c7pKxVlG0nIdZQ1TqS+/uEr093YjxCyuVcyT1kM54nXhcV9X3NCuywl15mDXKK7qPOZr3mCzvlCVHUo0EN71zkYODtQUHD+s4le5aPnTkJQaffLZqYrYuSezAgcPaRuES5lk5YW5TlZ5DqxLBqCR6+Xl/bw/P/u4Qw4dfqun9nQi3Imm7hLlZHqYyh9bYP6PKz+txL4gT4dZunDC3jlCPnEVW/b09ToRb2/KVh3WERt945y4qa3e+8rCOkOdcSGM/RO6isk7g4GEdYfXSBYyfUax2vT3dfPKyRe6iso7jbivrCMsX9/PBr22v++s+N3yY5Yv7HSys4zh4WFuZaMnS/ilMSZKVpwa3TuVuK2sb6WnWg9IQ22vv2smGbaVZ/acy/XsWnTqNvhk4eFgbqbZk6Qe/tp0lazcDcOOKMyd8jRO6X/5ISC8vBfupyxYx44Tuo9t6e7o7cs15szJ3W1nbmGg4bvkq5MYVZx5dK2SsLolH/u7iisc7r2E2mq88WkR5ydR5a+5hydrNR7ti7GWT5R+GD4+wbtNuLj93dsXt1crNbDwHjxYwWV++lUwlp/GLg8N8ZPmZvPu8OXSpNHi3S+Ld583hI8sn7tIqcyA3y2liREnrgLcBLwJ7gfdFxEFJc4FHgfIizVsi4qrkmLOBLwI9lBaD+kBMoXKdMDHikrWbK44S6vS7mCuNrAImnPywt6eb7ddfWNN7jp0jq6e7y/d2WMupdWLEvK487gNeHxH/DvgpcG1q296IWJT8XJUqvxl4PzA/+bkop7q1nGp9+Z281nW1qzGAH645n09dtojuY8bfFvi7F4/UdKVQLSm/btPuKkeYtadcgkdEfCcijiRPtwCzJtpf0kzg1RGxJbnauBVYnkfdWlG1vvxOvsdgsi/x5Yv7eeUrxo8HOTwSU/6ir9Q95UBuVtKInMefAN9KPZ8naZukf5X0xqSsH9iX2mdfUlaRpFWSBiUNHjhwoP41LphKffmdPn/SVL7ED75wONOxadWubF7T011x/04O5NaZpj1UV9L9wO9X2HRdRNyd7HMdcAT4SrJtPzAnIn6d5Dg2SFqY9b0jYj2wHko5j+nUv5WU+9Kr3TndiV7T083B4fHBIf0lfmqVO8qn8kVf7crmFd3H0NPdNS7n0cmB3DrTtINHRFww0XZJ7wX+CHhLOfEdEYeAQ8njrZL2AqcDQ4zu2pqVlFnC9xm8bMO2IX734pFx5WPv+F69dEHF5PZUvuirXZ0cfOEwN122yIHcOl4uNwlKugj4K+A/RsQLqfI+4NmIGJF0GqXE+OMR8ayk5yWdBzwAvAf4xzzqNl0TzZlkjbVu024Oj4y/4HzlK44ddU7SV2xDB4fpksblRaqZ6KrFgdwsv5zHPwGvAu6TtF3SZ5PyNwE7JG0HvgFcFRHPJtv+DPg8sIfS8N5vURC+z6K6ZtzzMNFVwVjLF/cfzRmV7yqfyvlznslsYrlceUTEH1QpvxO4s8q2QeD1edSnVhON7Onkv0DH3vOQHi6bZ7tkzWVM5/w5z2Q2Mc9tNQUenllZs4Jq1lzGdM+fu6fMqvP0JFPg+ywqa1ZQXb64nxtXnDnl1ft8/szqz1ceU1DLqJ12VstQ2KmYaJBClqsCnz+z+vOVxxRk/Uu3U+SZVK7nIAWfP7P6y2VixEbqhIkRiyyvIcyeDNIsX7VOjOhuK6tJXkllD1IwKzYHDyukqeRTfOOmWfM452GFNFk+xTdumjWXg4cV0mRJbq+rYdZc7rayuql3N9JE+ZRWy4m4i83ajYOHVZXlC6/RU5XkfY9JPTVrGhezPLnbyirKmlNodDdSK01c6C42a0cOHlZR1i+8RncjtdKNf63WxWY2Fe62soqyfuE1oxupVSYubKUuNrOp8pWHVZR1MsFW6kZqNLeNtSMHD6so6xdeK3UjNdryxf284+x+uiQAuiTecXZrXDWZVZNbt5WkG4D3AweSor+JiHuTbdcCVwIjwJ9HxKak/CLgH4Au4PMRsTav+tnEprMYUqt0IzXahm1D3Ll16OhKhiMR3Ll1iIHXnuT2spaVd87jpoj4eLpA0hnASmAhcCpwv6TTk82fAd4K7AMelLQxIh7JuY5WhYNBfXglSmtHzUiYLwNuj4hDwM8k7QHOSbbtiYjHASTdnuzr4GEtzaOtrB3lnfO4RtIOSbdImpGU9QNPpfbZl5RVKx9H0ipJg5IGDxw4UGkXs8LwSobWjmoKHpLul/RwhZ9lwM3A64BFwH7gE3WoLwARsT4iBiJioK+vr14va5YLj7aydlRTt1VEXDCV/SR9DviX5OkQMDu1eVZSxgTlZi1rOoMPzIouz9FWMyNif/L0UuDh5PFG4KuSPkkpYT4f+BEgYL6keZSCxkrgXXnVz6yRPPjA2k2eCfOPSVoEBPAE8KcAEbFL0h2UEuFHgKsjYgRA0jXAJkpDdW+JiF051s/MzKbJa5ibmXUgr2FuliOvw2FWmYOHWRVeh8OsOs9tZVaF1+Ewq87Bw6wK3xluVp2Dh1kVvjPcrDoHjza1YdsQS9ZuZt6ae1iydnPV5WOtOt8ZbladE+YtZKojf5zorQ/fGW5Wne/zaBFjAwKU/gq+ccWZwOgvuN8dOsLB4cPjXqO/t4cfrjm/YXU2s+LyfR4dotrInxs27uLQkZdGXWVU40SvmdWLg0eLqPbFX+kKoxones2sXpwwbxG1fvE70Wtm9eTg0SKqjfyZcUJ3xf1nnNBNf28PopTruHHFmU70mlnduNuqgnrNZ1TPeZGqjfwBKibSr3/bQgcLM8uNg8cY9Rrmmsdw2YnWhPBwUjNrJAePMSaazyjLF3K9XmcqvNCQmTWacx5j1Gs+I8+LZGbtLJcrD0lfA8pDe3qBgxGxSNJc4FGgPC3ploi4KjnmbOCLQA9wL/CBaMIdjKf29lS8VyLraKesr+N1I8ysleRy5RERl0XEoohYBNwJ3JXavLe8rRw4EjcD76e0pvl84KI86jaZes1nlOV1yvmRoYPDBC/nRzwflZkVVa7dVpIEvBO4bZL9ZgKvjogtydXGrcDyPOtWzfLF/dy44syah7lmeR2vG2FmrSbvhPkbgacj4rFU2TxJ24DngQ9FxP8G+oF9qX32JWUVSVoFrAKYM2dO3StdrwT0VF/H+REzazXTDh6S7gd+v8Km6yLi7uTx5Yy+6tgPzImIXyc5jg2SFmZ974hYD6yH0sSIWY8vmnrlWczMGmXawSMiLphou6RjgRXA2aljDgGHksdbJe0FTgeGgFmpw2clZR1h9dIF4270A9j/3DBz19xDvxPoZlYweeY8LgB+EhFHu6Mk9UnqSh6fRikx/nhE7Aeel3Rekid5D3B3pRdtR+X8SG/P6KlGXkquqZxAN7OiyTN4rGR8ovxNwA5J24FvAFdFxLPJtj8DPg/sAfYC38qxboWzfHE/Jx5f/ULQCXQzK5LcEuYR8d4KZXdSGrpbaf9B4PV51acVTJYgdwK9dfk+Hms3np6kQKolztPba+EvsObYsG2I1V9/iMNJP+TQwWFWf/0hwMsCW+vy9CQFUunGwrJa1+PwjYjNc8PGXUcDR9nhl4IbNu5qUo3MaufgUSDpGwsBuiSgPutx+EbE5qm22mOWVSDNisbdVgWT1wy5vhHRzOrJVx4dolq+xDci5m+i1R7NWpWDR4eo14SPlt31b1tId5dGlXV3ievflnlyBbPCcLdVh6i2jK1H++TPbW/tSE1YMqOuBgYGYnBwsNnVMDNrKZK2RsTAdI93t5WZmWXm4GFmZpk5eJiZWWYOHmZmlplHWzWB55jqPD7n1m4cPBqsPMdUeaqQ8hxT4Eny2pXPubUjd1s1mOeY6jw+59aOOvLKo5ldCK0+x5S7X7Jr9XNuVklNVx6S/ljSLkkvSRoYs+1aSXsk7Za0NFV+UVK2R9KaVPk8SQ8k5V+TdFwtdaum2VOTt/IcU81uu1bVW2UOq2rlZq2g1m6rh4EVwA/ShZLOoLQM7ULgIuCfJXUl65d/BrgYOAO4PNkX4KPATRHxB8BvgCtrrFtFze5CaOU5pprddq2q2iQOLT65g3W4moJHRDwaEZW+OZYBt0fEoYj4GaV1yc9JfvZExOMR8SJwO7BMkoDzKa1rDvAlYHktdaum2V0I6TU7RH3W6miUZrddq3quyrod1crNWkFeOY9+YEvq+b6kDOCpMeXnAr8HHIyIIxX2H0fSKmAVwJw5czJVrNpSr43sNsprzY68FaHtWpHbzdrRpFceku6X9HCFn2WNqGAlEbE+IgYiYqCvry/Tsa3cbdRsbrvpcbtZO5r0yiMiLpjG6w4Bs1PPZyVlVCn/NdAr6djk6iO9f115euzpc9tNj9vN2lFdpmSX9H3gv0fEYPJ8IfBVSjmOU4HvAvMBAT8F3kIpODwIvCsidkn6OnBnRNwu6bPAjoj458ne21Oym5ll19Qp2SVdKmkf8B+AeyRtAoiIXcAdwCPAt4GrI2Ikuaq4BtgEPArckewL8NfAX0jaQykH8oVa6mZmZvnxYlBmZh3Ii0GZmVnDOXiYmVlmDh5mZpZZy+c8JB0Ansx42MnAr3KoTr24frVx/aavyHUD169W6fq9NiKy3SiX0vLBYzokDdaSKMqb61cb12/6ilw3cP1qVc/6udvKzMwyc/AwM7PMOjV4rG92BSbh+tXG9Zu+ItcNXL9a1a1+HZnzMDOz2nTqlYeZmdXAwcPMzDJru+DRSuuqJ6+5Pfl5QtL2pHyupOHUts+mjjlb0s6kTp9OVmHMhaQbJA2l6nFJalumtsypfusk/UTSDknflNSblBei/SrUt2FtM0EdZkv6nqRHks/JB5LyzOc6xzo+kZyj7ZLKM3WfJOk+SY8lv2ck5UrO457k/8FZOddtQaqNtkt6XtIHm9l+km6R9Iykh1NlmdtL0hXJ/o9JumLSN46ItvoB/i2wAPg+MJAqPwN4CDgemAfsBbqSn73AacBxyT5nJMfcAaxMHn8W+K851vsTwIeTx3OBh6vs9yPgPErT238LuDjHOt1Aaar9seWZ2zKn+l0IHJs8/ijw0SK135j3bWjbTFCPmcBZyeNXUVoi4Yys5zrnOj4BnDym7GPAmuTxmtS5viQ5j0rO6wMNbMsu4JfAa5vZfsCbgLPS/+ezthdwEvB48ntG8njGRO/bdlce0YLrqifv9U7gtkn2mwm8OiK2ROmM35pXnSaRqS3zqkREfCdeXrp4C6VFxKpqcvs1tG2qiYj9EfHj5PFvKS2NMNGqVNXOdaMto/QZhNGfxWXArVGyhdKicjMbVKe3AHsjYqIZLnJvv4j4AfBshffN0l5Lgfsi4tmI+A1wH3DRRO/bdsFjAv2MXz+9f4LyTOuq1+iNwNMR8ViqbJ6kbZL+VdIbk7L+pB5j65qna5LL21vKl75kb8tG+BNKf1GVFaX9yprZNhVJmgssBh5IirKc6zwF8B1JWyWtSspOiYj9yeNfAqc0sX5lKxn9B19R2g+yt1fmerZk8FAB11WvZop1vZzR/wn3A3MiYjHwF8BXJb26CfW7GXgdsCip0yfyqEMN9Svvcx1wBPhKUtSw9mtVkl4J3Al8MCKepwDnOuUNEXEWcDFwtaQ3pTcmV41NvcdApfzn24GvJ0VFar9R8mqvSdcwL6JooXXVJ6urpGOBFcDZqWMOAYeSx1sl7QVOT94/3TVT81rvU21LSZ8D/iV5mrUtc6ufpPcCfwS8JfmQNLT9MpiozRpKUjelwPGViLgLICKeTm2f6rnORUQMJb+fkfRNSt08T0uaGRH7k26WZ5pVv8TFwI/L7Vak9ktkba8h4A/HlH9/ojdoySuPadoIrJR0vKR5lNZU/xGlddTnqzSy6jhKl6Ibky+i7wH/OTn+CuDuHOp1AfCTiDjanSKpT1JX8vi0pK6PJ5ehz0s6L8mTvCenOpXrke47vhQoj+bI1JY51u8i4K+At0fEC6nyQrTfGA1tm2qSf/cXgEcj4pOp8qznOq/6nSjpVeXHlAZFPJzUozwCKP1Z3Ai8JxlFdB7wXKq7Jk+jeguK0n4pWdtrE3ChpBlJl9uFSVl19cz6F+GH0onbR+kvz6eBTalt11Ea7bCb1CgbSiMQfppsuy5VfhqlE72H0uXp8TnU94vAVWPK3gHsArYDPwbelto2QOk/5l7gn0hmCcipLf8nsBPYkfynmzndtsypfnso9dNuT34+W6T2q1DfhrXNBHV4A6UujB2pdrtkOuc6p/qdRml00kPJObwuKf894LvAY8D9wElJuYDPJPXbSWqEZY51PJFSz8RrUmVNaz9KQWw/cJjSd9+V02kvSnnDPcnP+yZ7X09PYmZmmXVSt5WZmdWJg4eZmWXm4GFmZpk5eJiZWWYOHmZmlpmDh5mZZebgYWZmmf1/7r/PoSB1SCYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def example_plot(ear_df):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range (len(ear_df)):\n",
    "        cury = ear_df.loc[i, 'eps_est']\n",
    "        curx = ear_df.loc[i, 'eps']\n",
    "        if cury != 0 and curx != 0:\n",
    "            y.append(cury)\n",
    "            x.append(curx)\n",
    "    return plt.scatter(x, y)\n",
    "\n",
    "(sum_df, div_df, ear_df) = load_data_pandas('dataset_summary.csv.gz', 'stocks_latest/dividends_latest.csv.gz', \n",
    "                                 'stocks_latest/earnings_latest.csv.gz')\n",
    "p = example_plot(ear_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "This graph tells us a few things about the eps vs expected eps. It vaguely tells us that the relationship betweenn the expected EPS and the actual EPS is around x = y which implies that the expected and actual are very similar. We can also try to graph this on a specific set of companies or company to see trends for them or a specific industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Overall, we can use even this metadata to make certain observation and predictions about trends in the market. Trying to get more insights about these statistics and discovering trends is all part of the process of getting comfortable with thinking about the stock market and potentially quant if that is also an interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
